{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d138b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('teks_tokenisasi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d616c0",
   "metadata": {},
   "source": [
    "# Bobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc760314",
   "metadata": {},
   "source": [
    "## 1. POS TAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f6120cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 851kB/s]                     \n",
      "2025-06-17 20:03:00 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2025-06-17 20:03:00 INFO: Downloading default packages for language: id (Indonesian) ...\n",
      "2025-06-17 20:03:01 INFO: File exists: C:\\Users\\HP\\stanza_resources\\id\\default.zip\n",
      "2025-06-17 20:03:06 INFO: Finished downloading models and saved to C:\\Users\\HP\\stanza_resources\n",
      "2025-06-17 20:03:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 432kB [00:00, 8.72MB/s]                    \n",
      "2025-06-17 20:03:07 INFO: Downloaded file to C:\\Users\\HP\\stanza_resources\\resources.json\n",
      "2025-06-17 20:03:07 WARNING: Language id package default expects mwt, which has been added\n",
      "2025-06-17 20:03:08 INFO: Loading these models for language: id (Indonesian):\n",
      "==========================\n",
      "| Processor | Package    |\n",
      "--------------------------\n",
      "| tokenize  | gsd        |\n",
      "| mwt       | gsd        |\n",
      "| pos       | gsd_charlm |\n",
      "==========================\n",
      "\n",
      "2025-06-17 20:03:08 INFO: Using device: cpu\n",
      "2025-06-17 20:03:08 INFO: Loading: tokenize\n",
      "2025-06-17 20:03:08 INFO: Loading: mwt\n",
      "2025-06-17 20:03:08 INFO: Loading: pos\n",
      "2025-06-17 20:03:11 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "stanza.download('id')\n",
    "nlp = stanza.Pipeline(\n",
    "    lang='id'\n",
    "    , processors='tokenize,pos'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "563b19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung frekuensi POS tag\n",
    "jumlah_pos_ai = Counter()\n",
    "jumlah_pos_nonai = Counter()\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    doc = nlp(row['teks'])\n",
    "    tags = [word.upos for sent in doc.sentences for word in sent.words]\n",
    "    \n",
    "    if row['label'] == 1:\n",
    "        jumlah_pos_ai.update(tags)\n",
    "    else:\n",
    "        jumlah_pos_nonai.update(tags)\n",
    "\n",
    "# Gabungkan semua POS tag yang muncul\n",
    "all_tags = set(jumlah_pos_ai) | set(jumlah_pos_nonai)\n",
    "data_pos_freq = pd.DataFrame({\n",
    "    'POS_Tag': list(all_tags)\n",
    "    , 'AI': [jumlah_pos_ai.get(tag, 0) for tag in all_tags]\n",
    "    , 'NonAI': [jumlah_pos_nonai.get(tag, 0) for tag in all_tags]\n",
    "    , 'Selisih': [jumlah_pos_ai.get(tag, 0) - jumlah_pos_nonai.get(tag, 0) for tag in all_tags]\n",
    "    , 'Total': [jumlah_pos_ai.get(tag, 0) + jumlah_pos_nonai.get(tag, 0) for tag in all_tags]\n",
    "})\n",
    "\n",
    "data_pos_freq.to_csv(\"frekuensi_pos_tag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81ecf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung Bobot min-max\n",
    "min_selisih = data_pos_freq['Selisih'].min()\n",
    "max_selisih = data_pos_freq['Selisih'].max()\n",
    "data_pos_freq['Bobot'] = (data_pos_freq['Selisih'] - min_selisih) / (max_selisih - min_selisih)\n",
    "\n",
    "# Simpan POS_Tag dan Bobot ke CSV\n",
    "data_pos_freq[['POS_Tag', 'Bobot']].to_csv('bobot_pos_tag.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c773d",
   "metadata": {},
   "source": [
    "## 1. STOPWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0eaec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "021a4e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Unduh stopwords dan tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Ambil stopword Bahasa Indonesia\n",
    "stop_words = set(stopwords.words('indonesian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ffe8ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi dan hitung frekuensi stopword\n",
    "jumlah_stopword_ai = Counter()\n",
    "jumlah_stopword_nonai = Counter()\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    tokens = word_tokenize(row['teks'].lower())\n",
    "    stopword_tokens = [t for t in tokens if t in stop_words]\n",
    "\n",
    "    if row['label'] == 1:\n",
    "        jumlah_stopword_ai.update(stopword_tokens)\n",
    "    else:\n",
    "        jumlah_stopword_nonai.update(stopword_tokens)\n",
    "\n",
    "# Gabungkan semua stopword yang muncul\n",
    "all_stopwords = set(jumlah_stopword_ai) | set(jumlah_stopword_nonai)\n",
    "\n",
    "# Buat DataFrame\n",
    "data_stopword = pd.DataFrame({\n",
    "    'Stopword': list(all_stopwords),\n",
    "    'AI': [jumlah_stopword_ai.get(sw, 0) for sw in all_stopwords],\n",
    "    'NonAI': [jumlah_stopword_nonai.get(sw, 0) for sw in all_stopwords]\n",
    "})\n",
    "\n",
    "# Hitung Selisih dan Total\n",
    "data_stopword['Selisih'] = data_stopword['AI'] - data_stopword['NonAI']\n",
    "data_stopword['Total'] = data_stopword['AI'] + data_stopword['NonAI']\n",
    "\n",
    "# Hitung Bobot (min-max normalisasi dari Selisih)\n",
    "min_selisih = data_stopword['Selisih'].min()\n",
    "max_selisih = data_stopword['Selisih'].max()\n",
    "data_stopword['Bobot'] = (data_stopword['Selisih'] - min_selisih) / (max_selisih - min_selisih)\n",
    "\n",
    "# Simpan ke CSV\n",
    "data_stopword[['Stopword', 'Bobot']].to_csv('bobot_stopword.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
